---
title: "DATA622 | Machine Learning and Big Data"
author: "Gabriella Martinez"
date: "3/5/2023"
output:
      html_document:
        toc: yes
        toc_float: yes
        theme: yeti
        highlight: kate
        font-family: "Arial"
        code_folding: show
---

# Assignment 1
**Pre-Work**  
1. Visit the following website and explore the range of sizes of this dataset (from 100 to 5 million records):  
https://excelbianalytics.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/ or  
(new) https://www.kaggle.com/datasets  
2. Select 2 files to download. Based on your computer's capabilities (memory, CPU), select 2 files you can handle (recommended one small, one large)  
3. Download the files  
4. Review the structure and content of the tables, and think about the data sets (structure, size, dependencies, labels, etc)  
5. Consider the similarities and differences in the two data sets you have downloaded  
6. Think about how to analyze and predict an outcome based on the datasets available  
7. Based on the data you have, think which two machine learning algorithms presented so far could be used to analyze the data  

**Deliverable**  
1. Essay (minimum 500 word document). Write a short essay explaining your selection of algorithms and how they relate to the data and what you are trying to do.  
2. Exploratory Analysis using R or Python (submit code + errors + analysis as notebook or copy/paste to document. Explore how to analyze and predict an outcome based on the data available. This will be an exploratory exercise, so feel free to show errors and warnings that raise during the analysis. Test the code with both datasets selected and compare the results.

**Answer questions such as:**  
1. Are the columns of your data correlated?  
2. Are there labels in your data? Did that impact your choice of algorithm?  
3. What are the pros and cons of each algorithm you selected?  
4. How your choice of algorithm relates to the datasets (was your choice of algorithm impacted by the datasets you chose)?  
5. Which result will you trust if you need to make a business decision?  
6. Do you think an analysis could be prone to errors when using too much data, or when using the least amount possible?  
7. How does the analysis between data sets compare?  
Develop your exploratory analysis of the data and the essay in the following 2 weeks.  

# Packages
```{r message=FALSE, warning=FALSE}
library(readr)
library(tidyverse)
library(tidymodels)
library(psych)
library(caret)
library(rpart)
library(rpart.plot)
library(corrplot)
library(RColorBrewer)
library(labelled)
```


# Load Data
```{r}
df1k <- read.csv("https://raw.githubusercontent.com/gabbypaola/DATA622/main/HW%201/1000%20Sales%20Records.csv")
df100k <- read.csv("https://raw.githubusercontent.com/gabbypaola/DATA622/main/HW%201/100000%20Sales%20Records.csv")
```

# Exploratory Data Analysis

First, exploratory data analysis is conducted to get acquainted with the two Sales datasets selected from https://excelbianalytics.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/. 

## Dimensions, Variable Types, Labels, Levels, and Frequencies

The dimensions of the 1k data set are checked. The dataset contains 1000 rows, and 14 columns, or predictors. There are 7 character, 2 integer, and 5 double type variables. The `glimpse()` output shows us the first few observation of each variable. There are two geographical variables one for Region, and another for each specific country. Item.Type refers to the type of items sold, Sales.Channel refers to the sales method, whether the sale was conducted online or offline, meaning an in-store purchase.  

The `look_for()` function from the labelled package shows us each variable, their label (if available), column type, as well as the values for any factor type variables. The function produces no labels, and no values as none of the variables are coded as factor types. 

^[https://cran.r-project.org/web/packages/labelled/vignettes/intro_labelled.html]
```{r}
print("Dimensions")
glimpse(df1k)
print("Labels")
look_for(df1k)
```

Next, using the `unique()` and `length()` functions are used to investigate the geographic variables. There are 7 Regions and 185 countries spanning the 1k dataset.
```{r}
unique(df1k$Region)
length(unique(df1k$Country))
```

The `table()` function is used to create contingency, or frequency tables of some of the character variables, Item.Type and Sales.Channel. The sorted frequency table shows the top three items of the total 12 types in the 1k dataset are beverages, vegetables, followed by office supplies.
```{r}
table(df1k$Item.Type)[order(table(df1k$Item.Type),decreasing = TRUE)]
table(df1k$Sales.Channel)
```

Following the EDA for the 1k dataset, is the EDA for the 100k dataset. The 100k dataset contains 100,000 rows and 14 columns. Similar to the 1k dataset, in the 100k dataset there are 7 character, 2 integer, and 5 double type variables. The `glimpse()` output shows us the first few observation of each variable. There are two geographical variables one for Region, and another for each specific country. Item.Type refers to the type of items sold, Sales.Channel refers to the sales method, whether the sale was conducted online or offline, meaning an in-store purchase.

The `look_for()` function from the labelled package shows us each variable, their label (if available), column type, as well as the values for any factor type variables. The function produces no labels, and no values as none of the variables are coded as factor types. 
```{r}
print("Dimensions")
glimpse(df100k)
print("Labels")
look_for(df100k)
```
As above, using the `unique()` and `length()` functions are used to investigate the geographic variables. There are also 7 Regions and 185 countries spanning the 100k dataset.
```{r}
unique(df100k$Region)
length(unique(df100k$Country))
```

Once again the `table()` function is used to create contingency, or frequency tables of some of the character variables, Item.Type and Sales.Channel in the 100k dataset. The sorted frequency table shows the top three items of the total 12 types in the 100k dataset are office supplies, cereal, and baby food.
```{r}
table(df100k$Item.Type)[order(table(df100k$Item.Type),decreasing = TRUE)]
table(df100k$Sales.Channel)
```

## Variable Dependencies and Definitions

Both datasets contain the same information, only differing in the amount of observations. As such, below are the variable dependencies and definitions that are applicable to both the 1k and 100k dataset.

`Total.Cost` = `Units.Sold` * `Unit.Cost`  
`Total.Revenue` = `Units.Sold` * `Unit.Price`  
`Total.Profit` = `Total.Revenue` - `Total.Cost` (where `Total.Cost` and `Total.Revenue` depend on `Units.Sold`, `Unit.Cost`, and `Unit.Price`)

`Order.Priority`: C(Critical), H(High), M(Medium), and L(Low)


## Missing Data

Next the data is checked for any missing values.

The below shows no missing values for the 1k dataset. 

```{r}
colSums(is.na(df1k))
```

As in the 1k dataset, there are no missing values for the 100k dataset. 

```{r}
colSums(is.na(df100k))
```

## Correlations

Subsequently, the correlations between numeric variables are examined. It is important to investigate the correlations between the independent variables to avoid multicollinearity. Multicollinearity occurs when two or more independent variables are highly correlated to one another. When two (or more) independent variables are highly correlated one cannot individually determine the impact of individual variables on the dependent variable.  

Multicollinearity can be a problem in a regression model when using algorithms such as OLS (ordinary least squares). This is because the estimated regression coefficients become unstable and difficult to interpret in the presence of multicollinearity.  

When multicollinearity is present, the estimated regression coefficients may become large and unpredictable, leading to unreliable inferences about the effects of the predictor variables on the response variable. Therefore, it is important to check for multicollinearity and consider using other regression techniques that can handle this problem, such as ridge regression or principal component regression or make a decision about dropping highly correlated independent variables.

```{r}
df1k_n <- df1k %>% 
  keep(is.numeric) %>% 
  select(-Order.ID)

describe(df1k_n, fast=TRUE) %>% 
  select(c(-vars,-n))
```

```{r}
df100k_n <- df100k %>% 
  keep(is.numeric) %>% 
  select(-Order.ID)

describe(df100k_n, fast=TRUE) %>% 
  select(c(-vars,-n))
```


```{r}
cor(df1k_n) %>%
  corrplot(tl.col = 'black', diag=FALSE, type="lower", 
           order="hclust", addCoef.col = "black",
           #mar argument fixes title positioning
           title="1k dataset Correlations", mar=c(0,0,1,0), 
           col=brewer.pal(n=10, name="RdYlBu"))

```

```{r}
cor(df100k_n) %>%
  corrplot(tl.col = 'black', diag=FALSE, type="lower", 
           order="hclust", addCoef.col = "black",
           title="100k dataset Correlations",mar=c(0,0,1,0),
           col=brewer.pal(n=10, name="RdYlBu"))

```


## Distributions

```{r message=FALSE, warning=FALSE}
df1k_n %>% 
  gather(variable, value, 1:6) %>%
  ggplot(aes(value)) +
    facet_wrap(~variable, scales = "free") +
    geom_density(fill = "steelblue", alpha=0.9, color="steelblue") +
    geom_histogram(aes(y=after_stat(density)), alpha=0.2, fill = "lightblue", 
                   color="lightblue", position="identity", bins = 40) +
    theme_minimal()
```




```{r message=FALSE, warning=FALSE}
df100k_n %>% 
  gather(variable, value, 1:6) %>%
  ggplot(aes(value)) +
    facet_wrap(~variable, scales = "free") +
    geom_density(fill = "steelblue", alpha=0.9, color="steelblue") +
    geom_histogram(aes(y=after_stat(density)), alpha=0.2, fill = "lightblue", 
                   color="lightblue", position="identity",bins = 40) +
    theme_minimal()
```
  

## Variable Type Conversions
```{r}
df1k[['Order.Date']] <- as.Date(df1k[['Order.Date']], "%m/%d/%Y")
df1k[['Ship.Date']] <- as.Date(df1k[['Ship.Date']], "%m/%d/%Y")

df100k[['Order.Date']] <- as.Date(df100k[['Order.Date']], "%m/%d/%Y")
df100k[['Ship.Date']] <- as.Date(df100k[['Ship.Date']], "%m/%d/%Y")

df1k[['Sales.Channel']] <- as.factor(df1k[['Sales.Channel']])
df100k[['Sales.Channel']] <- as.factor(df100k[['Sales.Channel']])

df1k[['Order.Priority']] <- as.factor(df1k[['Order.Priority']])
df100k[['Order.Priority']] <- as.factor(df100k[['Order.Priority']])

df1k[['Item.Type']] <- as.factor(df1k[['Item.Type']])
df100k[['Item.Type']] <- as.factor(df100k[['Item.Type']])
```

```{r}
levels(df1k$Sales.Channel)
levels(df100k$Sales.Channel)
```


# Models
## 
#### Model Perfomance


```{r}

```


# Essay
**Answer questions such as:**  
1. Are the columns of your data correlated?  
Yes, the numeric variables in both the 1k and 100k dataset are highly correlated with exception to `Units.Sold` correlations at 0 with `Unit.Cost` and `Unit.Price`. `Units.Sold` highest correlation is with `Total.Profit` which makes sense as `Total.Profit` has dependency on `Units.Sold`, `Total.Profit` = `Total.Revenue` - `Total.Cost` (where `Total.Cost` and `Total.Revenue` depend on `Units.Sold`, `Unit.Cost`, and `Unit.Price`). The highest correlations exist between `Unit.Cost` and `Unit.Price`, `Total.Revenue` and `Total.Profit`, as well as `Total.Revenue` and `Total.Cost`.
  
2. Are there labels in your data? Did that impact your choice of algorithm?  
No, both datasets do not contain labels in their metadata as seen by the output of the `look_for()` function from the `labelled` package.  
  
3. What are the pros and cons of each algorithm you selected?   
Two models were run for the binary classification of the `Sales.Channel` variable in both the 1k and 100k datasets. Decision Tree models were initially run on both datasets. The next models run on the data were Logistic Regression models. While the Decision Tree model provided highly interpretable results for the 1k dataset, it failed to provide a meaningful result on the 100k dataset. Although Decision Tree models in general are simple, highly interpretable and do not require normalization nor scaling of data, they do not perform well with large amounts of data. This dynamic can be seen in the resulting model using the 100k dataset which provided underwhelming results. Logistic Regression was the next model type chosen to be run on the data. Given the binary response variable, and no need to normalize the data, it was thought that logistic regression was the right way to go for ease and convenience. While the logistic model was able to handle both the small and large datasets, its accuracy was lacking. A major contributor to its lackluster performance can be attributed to the multicollinearity noted among the independent variables in the Exploratory Data Analysis section. Unfortunately, correlations between variables were checked *after* running the logistic regression models, which was an error on my behalf in the model selection process. 

^[https://dhirajkumarblog.medium.com/top-5-advantages-and-disadvantages-of-decision-tree-algorithm-428ebd199d9a]
  
4. How your choice of algorithm relates to the datasets (was your choice of algorithm impacted by the datasets you chose)?   
The classification algorithms were chosen as a result of noting the binary `Sales.Channel` variable, however other models could have been run on continuous variables such as `Total.Revenue` or `Total.Profit`.
  
5. Which result will you trust if you need to make a business decision?  
6. Do you think an analysis could be prone to errors when using too much data, or when using the least amount possible?  
7. How does the analysis between data sets compare?  
Develop your exploratory analysis of the data and the essay in the following 2 weeks.  


# References

<!------- Below is for removing excessive space in Rmarkdown | HTML formatting -------->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>