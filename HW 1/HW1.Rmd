---
title: "DATA622 | Machine Learning and Big Data"
author: "Gabriella Martinez"
date: "3/5/2023"
output:
      html_document:
        toc: yes
        toc_float: yes
        theme: yeti
        highlight: kate
        font-family: "Arial"
        code_folding: show
---

# Assignment 1
**Pre-Work**  
1. Visit the following website and explore the range of sizes of this dataset (from 100 to 5 million records):  
https://excelbianalytics.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/ or  
(new) https://www.kaggle.com/datasets  
2. Select 2 files to download. Based on your computer's capabilities (memory, CPU), select 2 files you can handle (recommended one small, one large)  
3. Download the files  
4. Review the structure and content of the tables, and think about the data sets (structure, size, dependencies, labels, etc)  
5. Consider the similarities and differences in the two data sets you have downloaded  
6. Think about how to analyze and predict an outcome based on the datasets available  
7. Based on the data you have, think which two machine learning algorithms presented so far could be used to analyze the data  

**Deliverable**  
1. Essay (minimum 500 word document). Write a short essay explaining your selection of algorithms and how they relate to the data and what you are trying to do.  
2. Exploratory Analysis using R or Python (submit code + errors + analysis as notebook or copy/paste to document. Explore how to analyze and predict an outcome based on the data available. This will be an exploratory exercise, so feel free to show errors and warnings that raise during the analysis. Test the code with both datasets selected and compare the results.

**Answer questions such as:**  
1. Are the columns of your data correlated?  
2. Are there labels in your data? Did that impact your choice of algorithm?  
3. What are the pros and cons of each algorithm you selected?  
4. How your choice of algorithm relates to the datasets (was your choice of algorithm impacted by the datasets you chose)?  
5. Which result will you trust if you need to make a business decision?  
6. Do you think an analysis could be prone to errors when using too much data, or when using the least amount possible?  
7. How does the analysis between data sets compare?  
Develop your exploratory analysis of the data and the essay in the following 2 weeks.  

# Packages
```{r message=FALSE, warning=FALSE}
library(readr)
library(tidyverse)
library(tidymodels)
library(psych)
library(caret)
library(rpart)
library(rpart.plot)
library(corrplot)
library(RColorBrewer)
library(labelled)
library(ggplot2)
library(ggforce)
```


# Load Data
```{r}
df1k <- read.csv("https://raw.githubusercontent.com/gabbypaola/DATA622/main/HW%201/1000%20Sales%20Records.csv")
df100k <- read.csv("https://raw.githubusercontent.com/gabbypaola/DATA622/main/HW%201/100000%20Sales%20Records.csv")
```

# Exploratory Data Analysis

First, exploratory data analysis is conducted to get acquainted with the two Sales datasets selected from https://excelbianalytics.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/. 

## Dimensions, Variable Types, Labels, Levels, and Frequencies

The dimensions of the 1k data set are checked. The dataset contains 1000 rows, and 14 columns, or predictors. There are 7 character, 2 integer, and 5 double type variables. The `glimpse()` output shows us the first few observation of each variable. There are two geographical variables one for Region, and another for each specific country. Item.Type refers to the type of items sold, Sales.Channel refers to the sales method, whether the sale was conducted online or offline, meaning an in-store purchase.  

The `look_for()` function from the labelled package shows us each variable, their label (if available), column type, as well as the values for any factor type variables. The function produces no labels, and no values as none of the variables are coded as factor types. 

^[https://cran.r-project.org/web/packages/labelled/vignettes/intro_labelled.html]
```{r}
print("Dimensions")
glimpse(df1k)
print("Labels")
look_for(df1k)
```

Next, using the `unique()` and `length()` functions are used to investigate the geographic variables. There are 7 Regions and 185 countries spanning the 1k dataset.
```{r}
unique(df1k$Region)
length(unique(df1k$Country))
```

The `table()` function is used to create contingency, or frequency tables of some of the character variables, Item.Type and Sales.Channel. The sorted frequency table shows the top three items of the total 12 types in the 1k dataset are beverages, vegetables, followed by office supplies.
```{r}
table(df1k$Item.Type)[order(table(df1k$Item.Type),decreasing = TRUE)]
table(df1k$Sales.Channel)
```

Following the EDA for the 1k dataset, is the EDA for the 100k dataset. The 100k dataset contains 100,000 rows and 14 columns. Similar to the 1k dataset, in the 100k dataset there are 7 character, 2 integer, and 5 double type variables. The `glimpse()` output shows us the first few observation of each variable. There are two geographical variables one for Region, and another for each specific country. Item.Type refers to the type of items sold, Sales.Channel refers to the sales method, whether the sale was conducted online or offline, meaning an in-store purchase.

The `look_for()` function from the labelled package shows us each variable, their label (if available), column type, as well as the values for any factor type variables. The function produces no labels, and no values as none of the variables are coded as factor types. 
```{r}
print("Dimensions")
glimpse(df100k)
print("Labels")
look_for(df100k)
```
As above, using the `unique()` and `length()` functions are used to investigate the geographic variables. There are also 7 Regions and 185 countries spanning the 100k dataset.
```{r}
unique(df100k$Region)
length(unique(df100k$Country))
```

Once again the `table()` function is used to create contingency, or frequency tables of some of the character variables, Item.Type and Sales.Channel in the 100k dataset. The sorted frequency table shows the top three items of the total 12 types in the 100k dataset are office supplies, cereal, and baby food.
```{r}
table(df100k$Item.Type)[order(table(df100k$Item.Type),decreasing = TRUE)]
table(df100k$Sales.Channel)
```

## Variable Dependencies and Definitions

Both datasets contain the same information, only differing in the amount of observations. As such, below are the variable dependencies and definitions that are applicable to both the 1k and 100k dataset.

`Total.Cost` = `Units.Sold` * `Unit.Cost`  
`Total.Revenue` = `Units.Sold` * `Unit.Price`  
`Total.Profit` = `Total.Revenue` - `Total.Cost` (where `Total.Cost` and `Total.Revenue` depend on `Units.Sold`, `Unit.Cost`, and `Unit.Price`)

`Order.Priority`: C(Critical), H(High), M(Medium), and L(Low)

## Variable Type Conversions

Variable types are converted to ensure the models created use the correct type of variables. Variable conversion also aids in the preprocessing by ensuring originally numeric variables such as `Order.ID` are not inadvertently normalized or used in the modelling stage. There are a number of advantages to converting categorical variables to factor variables. Perhaps the most important advantage is that they can be used in statistical modeling where they will be implemented correctly, i.e., they will then be assigned the correct number of degrees of freedom. ^[https://stats.oarc.ucla.edu/r/modules/factor-variables/#:~:text=Perhaps%20the%20most%20important%20advantage,many%20different%20types%20of%20graphics.]

```{r}
df1k[['Order.Date']] <- as.Date(df1k[['Order.Date']], "%m/%d/%Y")
df1k[['Ship.Date']] <- as.Date(df1k[['Ship.Date']], "%m/%d/%Y")

df100k[['Order.Date']] <- as.Date(df100k[['Order.Date']], "%m/%d/%Y")
df100k[['Ship.Date']] <- as.Date(df100k[['Ship.Date']], "%m/%d/%Y")

df1k[['Sales.Channel']] <- as.factor(df1k[['Sales.Channel']])
df100k[['Sales.Channel']] <- as.factor(df100k[['Sales.Channel']])

df1k[['Order.Priority']] <- as.factor(df1k[['Order.Priority']])
df100k[['Order.Priority']] <- as.factor(df100k[['Order.Priority']])

df1k[['Item.Type']] <- as.factor(df1k[['Item.Type']])
df100k[['Item.Type']] <- as.factor(df100k[['Item.Type']])

df1k[['Region']] <- as.factor(df1k[['Region']])
df100k[['Region']] <- as.factor(df100k[['Region']])

df1k[['Country']] <- as.factor(df1k[['Country']])
df100k[['Country']] <- as.factor(df100k[['Country']])

df1k[['Order.ID']] <- as.character(df1k[['Order.ID']])
df100k[['Order.ID']] <- as.character(df100k[['Order.ID']])
```

```{r}
levels(df1k$Sales.Channel)
levels(df100k$Sales.Channel)
```


## Missing Data

Next the data is checked for any missing values.

The below shows no missing values for the 1k dataset. 

```{r}
colSums(is.na(df1k))
```

As in the 1k dataset, there are no missing values for the 100k dataset. 

```{r}
colSums(is.na(df100k))
```






## Distributions
```{r}
#select numeric columns 1k
df1k_n <- df1k %>% 
   keep(is.numeric) 
  #  %>% select(-Order.ID)

#select numeric columns 100k
df100k_n <- df100k %>% 
  keep(is.numeric) 
# %>% 
#   select(-Order.ID)
```

From the `describe()` output for the 1k dataset, it is noted that the numeric variables have wide ranges. This will be taken care of through normalization to scale the variables in the preprocessing stage prior to running models. Futhermore, through the use of the graphs created for each, it is noted that `Total.Cost`, `Total.Profit`, and `Total.Revenue` are right skewed and the distributions for `Unit.Cost`, `Unit.Price`, and `Units.Sold` are multimodal. `Unit.Cost` shows four modes, `Unit.Price` shows three modes, and `Units.Sold` shows two modes. 


```{r message=FALSE, warning=FALSE}
#stats
describe(df1k_n, fast=TRUE) %>% 
  select(c(-vars,-n))

#distribution
df1k_n %>% 
  gather(variable, value, 1:6) %>%
  ggplot(aes(value)) +
    facet_wrap(~variable, scales = "free") +
    geom_density(fill = "steelblue", alpha=0.9, color="steelblue") +
    geom_histogram(aes(y=after_stat(density)), alpha=0.2, fill = "lightblue", 
                   color="lightblue", position="identity", bins = 40) +
    theme_minimal()
```

Similarly, from the `describe()` output for the 100k dataset, it is noted that the numeric variables also have wide ranges. This will be taken care of through normalization to scale the variables in the preprocessing stage prior to running models. Futhermore, through the use of the graphs created for each variable, it is noted that `Total.Cost`, `Total.Profit`, and `Total.Revenue` are right skewed and the distributions for `Unit.Cost`, `Unit.Price`, and `Units.Sold` are multimodal. `Unit.Cost` shows six modes, `Unit.Price` shows five modes, and `Units.Sold` show four modes. 

```{r message=FALSE, warning=FALSE}
#stats
describe(df100k_n, fast=TRUE) %>% 
  select(c(-vars,-n))

#distributions
df100k_n %>% 
  gather(variable, value, 1:6) %>%
  ggplot(aes(value)) +
    facet_wrap(~variable, scales = "free") +
    geom_density(fill = "steelblue", alpha=0.9, color="steelblue") +
    geom_histogram(aes(y=after_stat(density)), alpha=0.2, fill = "lightblue", 
                   color="lightblue", position="identity",bins = 40) +
    theme_minimal()
```
  

## Correlations

Subsequently, the correlations between numeric variables are examined. It is important to investigate the correlations between the independent variables to avoid multicollinearity. Multicollinearity occurs when two or more independent variables are highly correlated to one another. When two (or more) independent variables are highly correlated one cannot individually determine the impact of individual variables on the dependent variable.  

Multicollinearity can be a problem in a regression model when using algorithms such as OLS (ordinary least squares). This is because the estimated regression coefficients become unstable and difficult to interpret in the presence of multicollinearity.  

When multicollinearity is present, the estimated regression coefficients may become large and unpredictable, leading to unreliable inferences about the effects of the predictor variables on the response variable. Therefore, it is important to check for multicollinearity and consider using other regression techniques that can handle this problem, such as ridge regression or principal component regression or make a decision about dropping highly correlated independent variables entirely.

```{r}
cor(df1k_n) %>%
  corrplot(tl.col = 'black', diag=FALSE, type="lower", 
           order="hclust", addCoef.col = "black",
           #mar argument fixes title positioning
           title="1k dataset Correlations", mar=c(0,0,1,0), 
           col=brewer.pal(n=10, name="RdYlBu"))

```

```{r}
cor(df100k_n) %>%
  corrplot(tl.col = 'black', diag=FALSE, type="lower", 
           order="hclust", addCoef.col = "black",
           title="100k dataset Correlations",mar=c(0,0,1,0),
           col=brewer.pal(n=10, name="RdYlBu"))

```

The correlation plots for both the 1k and 100k dataset show nearly identical correlation coefficients. Variables with weak correlation (i.e., 0<=|r|<0.3) ^[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4888898/] between Unit.Price, Unit.Cost, and Units.Sold. Moderate correlation (i.e., 0.3≤|r|<0.7) can be seen between Total.Profit, Total.Revenue, Total.Cost and Units.Sold as well as between Unit.Price, Unit.Cost and Total.Profit. High correlation (i.e., |r|>=0.7) can be seen in the remaining pairs, especially between Unit.Price and Unit.Cost, Total.Profit and Total.Revenue, Total.Cost and Total.Revenue.

## VIF Scores

Another detection method for multicoliniarity is through the use of the VIF (Variance Inflation Factor) score.  The variance inflation factor (or VIF), measures how much the variance of a regression coefficient is inflated due to multicollinearity in the model. The smallest possible value of VIF is one (absence of multicollinearity). As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. ^[http://www.sthda.com/english/articles/39-regression-model-diagnostics/160-multicollinearity-essentials-and-vif-in-r/]

```{r}
set.seed(145)

training.samples <- df1k_n$Total.Revenue %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- df1k_n[training.samples, ]
test.data <- df1k_n[-training.samples, ]
```

```{r}
# Build the model
#train.data1 <-train.data #%>% select(c(-Total.Profit,-Total.Cost, -Unit.Price)) 
model1<- lm(Total.Revenue~., data=train.data )

summary(model1)
# Make predictions
predictions <- model1 %>% predict(test.data)
# Model performance
data.frame(
  RMSE = RMSE(predictions, test.data$Total.Revenue),
  R2 = R2(predictions, test.data$Total.Revenue)
)
```

As expected from the correlation plot, multicoliniarity is detected through the resulting VIF scores for the 1k dataset. Four of the five numeric variables have VIF scores higher than 10, two of which are exorbitantly high with VIF scores of over 170 (`Unit.Price` and `Unit.Cost`) as seen below.

```{r}
car::vif(model1)
```

Similarly, the VIF scores are calculated for the 100k dataset.

```{r}
set.seed(145)

training.samples2 <- df100k_n$Total.Revenue %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data2  <- df100k_n[training.samples2, ]
test.data2 <- df100k_n[-training.samples2, ]
```

```{r}
# Build the model
#%>% select(c(-Total.Profit,-Total.Cost, -Unit.Price)) 
model2<- lm(Total.Revenue~., data=train.data2 )

summary(model2)
# Make predictions
predictions2 <- model2 %>% predict(test.data2)
# Model performance
data.frame(
  RMSE = RMSE(predictions2, test.data2$Total.Revenue),
  R2 = R2(predictions2, test.data2$Total.Revenue)
)
```

As in the VIF scores for the 1k dataset, the 100k dataset exhibits high VIF values, confirming the multicoliniairty detected between the predictor variables in the correlation plot.

```{r}
car::vif(model1)
```

In both the 1k and 100k datasets, the only variable not exceeding the recommended VIF score of 5 is the Units.Sold variable. As such, it will be the only numeric variable used in the modeling.


# Preprocessing
## Normalization

Next, the data is normalized in preparation for modelling. ^[https://www.statology.org/how-to-normalize-data-in-r/] When variables are measured at different scales, they often do not contribute equally to the analysis. For example, if the values of one variable range from 0 to 100,000 and the values of another variable range from 0 to 100, the variable with the larger range will be given a larger weight in the analysis. By normalizing the variables, we can be sure that each variable contributes equally to the analysis.

```{r}
#define Min-Max normalization function
min_max_norm <- function(x) {
    (x - min(x)) / (max(x) - min(x))
}

#apply Min-Max normalization to numeric columns
df1k_norm <- as.data.frame(lapply(df1k_n, min_max_norm))

df100k_norm <- as.data.frame(lapply(df100k_n, min_max_norm))
```

```{r message=FALSE, warning=FALSE}
#stats
describe(df1k_norm, fast=TRUE) %>% 
  select(c(-vars,-n))

#distribution
df1k_norm %>% 
  gather(variable, value, 1:6) %>%
  ggplot(aes(value)) +
    facet_wrap(~variable, scales = "free") +
    geom_density(fill = "steelblue", alpha=0.9, color="steelblue") +
    geom_histogram(aes(y=after_stat(density)), alpha=0.2, fill = "lightblue", 
                   color="lightblue", position="identity", bins = 40) +
    theme_minimal()
```

```{r message=FALSE, warning=FALSE}
#stats
describe(df100k_norm, fast=TRUE) %>% 
  select(c(-vars,-n))

#distribution
df100k_norm %>% 
  gather(variable, value, 1:6) %>%
  ggplot(aes(value)) +
    facet_wrap(~variable, scales = "free") +
    geom_density(fill = "steelblue", alpha=0.9, color="steelblue") +
    geom_histogram(aes(y=after_stat(density)), alpha=0.2, fill = "lightblue", 
                   color="lightblue", position="identity", bins = 40) +
    theme_minimal()
```

# Models
## Simple Linear Regression

The first model created is a simple linear regression using the normalized numeric data for the 1k dataset.

First the data is split into a train and test sets.
```{r}
set.seed(146)

training.samples3 <- df1k_norm$Total.Revenue %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data3  <- df1k_norm[training.samples3, ]
test.data3 <- df1k_norm[-training.samples3, ]
```

Next, the model is run using the predictor variable, `Units.Sold` identified in the EDA stage from the VIF Scores calculated.

The model performs terribly with an R-squared value of .268
```{r}
model3<- lm(Total.Revenue~Units.Sold, data=train.data3 )

summary(model3)
# Make predictions
predictions3 <- model3 %>% predict(test.data3)

# Model performance
data.frame(
  MAE = mae(predictions3, test.data3$Total.Revenue),
  RMSE = RMSE(predictions3, test.data3$Total.Revenue),
  R2 = R2(predictions3, test.data3$Total.Revenue)
)
```

Another simple regression model is created, this time with the 100k dataset using the same predictor and response variables as in the model created using the 1k dataset.

```{r}
set.seed(146)

training.samples4 <- df100k_norm$Total.Revenue %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data4  <- df1k_norm[training.samples4, ]
test.data4 <- df1k_norm[-training.samples4, ]
```

```{r}
model4<- lm(Total.Revenue~Units.Sold, data=train.data4)

summary(model4)
# Make predictions
predictions4 <- model4 %>% predict(test.data4)

# Model performance
data.frame(
  MAE = mae(predictions4, test.data4$Total.Revenue),
  RMSE = RMSE(predictions4, test.data4$Total.Revenue),
  R2 = R2(predictions4, test.data4$Total.Revenue)
)
```
As in the simple regression model using the 1k dataset, the 100k dataset doesn't perform well. It has an accuracy of 26% which is nearly 1% worse than the model performance for the 1k dataset.

## Multiple Linear Regression

For the next model, additional predictor variables are incorporated in the `lm()` function to generate a multiple linear regression model for both the 1k  and 100k datasets.

First step is to narrow down the variables of the 1k dataset to be used and then preprocess the numeric values to create the model.
```{r}
df1k2 <- df1k %>% 
  # purrr::discard(is.numeric) %>% 
  select(-c(Country, Unit.Cost, Unit.Price, Total.Cost, Total.Profit, Order.ID))

preproc1 <-preProcess(df1k2, method=c("center", "scale"))
norm1 <- predict(preproc1,df1k2)
head(norm1)
```

```{r}
set.seed(146)

training.samples5 <- norm1$Total.Revenue %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data5  <- norm1[training.samples5, ]
test.data5 <- norm1[-training.samples5, ]
```


```{r}
model5<- lm(Total.Revenue~., data=train.data5 )

summary(model5)
# Make predictions
predictions5 <- model5 %>% predict(test.data5)

# Model performance
data.frame(
  MAE = mae(predictions3, test.data3$Total.Revenue),
  RMSE = RMSE(predictions5, test.data5$Total.Revenue),
  R2 = R2(predictions5, test.data5$Total.Revenue)
)
```
A significant improvement in the accuracy of the model is shown by the R squared value which is now 0.81.


Next is the multiple linear regression model for the 100k dataset. 
```{r}
df100k2 <- df100k %>% 
  # purrr::discard(is.numeric) %>% 
  select(-c(Country, Unit.Cost, Unit.Price, Total.Cost, Total.Profit, Order.ID))

preproc2 <-preProcess(df100k2, method=c("center", "scale"))
norm2 <- predict(preproc2,df100k2)
head(norm2)
```

```{r}
set.seed(147)

training.samples6 <- norm2$Total.Revenue %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data6  <- norm2[training.samples6, ]
test.data6 <- norm2[-training.samples6, ]
```

```{r}
model6<- lm(Total.Revenue~., data=train.data6 )

summary(model6)
# Make predictions
predictions6 <- model6 %>% predict(test.data6)

# Model performance
data.frame(
  MAE = mae(predictions6, test.data6$Total.Revenue),
  RMSE = RMSE(predictions6, test.data6$Total.Revenue),
  R2 = R2(predictions6, test.data6$Total.Revenue)
)
```

# Essay
**Answer questions such as:**  
**1. Are the columns of your data correlated?**  
Yes, the numeric variables in both the 1k and 100k dataset are highly correlated which were verified using correlation plots as well as VIF scores. A majority of the numerical variables are highly correlated. The two variable relationships that have a 0 correlation are between `Units.Sold` and `Unit.Price`, as well as between `Units.Sold` and `Unit.Cost`. Moderate correlations (i.e., 0.3≤|r|<0.7) exist between the following pairs: `Units.Sold` and `Total.Profit`, `Units.Sold` and `Total.Revenue`, `Units.Sold` and `Total.Cost`, `Unit.Price` and `Total.Profit`, and `Unit.Cost` and `Total.Profit`. The highest correlations with values at .99 exist between `Unit.Price` and `Unit.Cost` as well as `Total.Cost` and `Total.Revenue`. The remaining six numeric variable relationships have high correlations (i.e., |r|>=0.7). Multicollinearity was expected between the numeric variables given the variable dependencies noted in the exploratory data analysis portion, however both the correlation plots and the VIF scores confirmed this expectation.  
**2. Are there labels in your data? Did that impact your choice of algorithm?**  
No, both datasets do not contain labels in their metadata as seen by the output of the `look_for()` function from the `labelled` package.  
**3. What are the pros and cons of each algorithm you selected?**  
The pro of the simple linear regression algorithm was its easy interpretation and implementation, however its major con is its poor accuracy. The simple linear regression returned an R squared value of approximately 0.27 for the 1k dataset, and an R squared value of approximately 0.26 for the 100k dataset. In terms of the multiple linear regression model, it also has easy implementation but is slightly more complicated to interpret in contrast to the simple linear regression model. The slight complexity arises in the multiple linear regression model because of the addition of the factor type predictor variables with multiple levels. Given this dynamic, the factor / categorical variable `Country` was not included in the model because it would have resulted in a summary model output with 185 coefficients. The addition of the categorical and date variables significantly improved both models generated for each dataset. The multiple linear regression model run with the 1k dataset returned an R squared value of 0.81. The multiple linear regression model using the 100k dataset also saw improvement with an R squared value of 0.82. Although both multiple linear regression models for each dataset saw major improvements from their simple linear regression models with the addition of more predictor variables, both still have room for improvement, particularly to have a higher accuracy with an R squared value in the 0.90’s.   
**4. How your choice of algorithm relates to the datasets (was your choice of algorithm impacted by the datasets you chose)?**    
The choice of algorithm related heavily on the type of datasets chosen from the https://excelbianalytics.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/ website. Initially, the first model chosen for this assignment was a classification model of the variable `Sales.Channel`. The models run were decision tree and random forest for each dataset. In the first instance, it was thought that using random forest models would provide highly improved results in contrast to decision tree models, however that was not the case. Both models for each dataset returned terrible results, both the decision tree and random forest model returning R squared values of approximately 0.50, and some even less. Given the lackluster results, it was decided to take another approach, and instead perform a regression analysis on the `Total.Revenue` variable and use simple linear regression as well as multiple linear regression. This choice proved to be much better over the classification analysis using the tree based models.  
**5. Which result will you trust if you need to make a business decision?**  
Of the two models ran for this assignment, if I had to trust one to make a business decision, I would go with the multiple linear regression. While it can still benefit from some improvements, it performed much better than the simple linear regression model. The improvement in performance can most definitely be attributed to the addition of predictor variables as they provided more information and data for the model to predict the `Total.Revenue` variable. Some potential improvements that can be implemented to increase its accuracy would be to perform feature engineering or clustering common data points.  
**6. Do you think an analysis could be prone to errors when using too much data, or when using the least amount possible?**  
Using this assignment as the basis for my response, I’d say in this case, the addition of data didn’t vary the outcomes for each model run. Both models run using the 1k dataset and the 100k dataset produced approximately the same R squared values, only varying by 0.01. However, this may not be the case in other circumstances with different datasets. It is also important to take into consideration what it means to add more data, i.e. more observations (rows) as in this case, or more variables (columns). For this assignment, the dataset dimensions differed only in their observation counts, their variables were exactly the same.  
**7. How does the analysis between data sets compare?**  
The analysis between the two datasets are relatively the same, only differing in their R squared scores by 0.01. For the simple regression models, the model using the 1k dataset performed better than that run using the 100k dataset by about 0.01. In similar fashion, the multiple linear regression models also differed in R squared values by 0.01, however in this case, the model using the 100k dataset performed better by 0.01, which isn't much of a meaningful difference.
  

Develop your exploratory analysis of the data and the essay in the following 2 weeks.  


# References

<!------- Below is for removing excessive space in Rmarkdown | HTML formatting -------->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>