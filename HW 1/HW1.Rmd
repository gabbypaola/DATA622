---
title: "DATA622 | Machine Learning and Big Data"
author: "Gabriella Martinez"
date: "3/5/2023"
output:
      html_document:
        toc: yes
        toc_float: yes
        theme: yeti
        highlight: kate
        font-family: "Arial"
        code_folding: show
---

# Assignment 1
**Pre-Work**  
1. Visit the following website and explore the range of sizes of this dataset (from 100 to 5 million records):  
https://excelbianalytics.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/ or  
(new) https://www.kaggle.com/datasets  
2. Select 2 files to download. Based on your computer's capabilities (memory, CPU), select 2 files you can handle (recommended one small, one large)  
3. Download the files  
4. Review the structure and content of the tables, and think about the data sets (structure, size, dependencies, labels, etc)  
5. Consider the similarities and differences in the two data sets you have downloaded  
6. Think about how to analyze and predict an outcome based on the datasets available  
7. Based on the data you have, think which two machine learning algorithms presented so far could be used to analyze the data  

**Deliverable**  
1. Essay (minimum 500 word document). Write a short essay explaining your selection of algorithms and how they relate to the data and what you are trying to do.  
2. Exploratory Analysis using R or Python (submit code + errors + analysis as notebook or copy/paste to document. Explore how to analyze and predict an outcome based on the data available. This will be an exploratory exercise, so feel free to show errors and warnings that raise during the analysis. Test the code with both datasets selected and compare the results.

**Answer questions such as:**  
1. Are the columns of your data correlated?  
2. Are there labels in your data? Did that impact your choice of algorithm?  
3. What are the pros and cons of each algorithm you selected?  
4. How your choice of algorithm relates to the datasets (was your choice of algorithm impacted by the datasets you chose)?  
5. Which result will you trust if you need to make a business decision?  
6. Do you think an analysis could be prone to errors when using too much data, or when using the least amount possible?  
7. How does the analysis between data sets compare?  
Develop your exploratory analysis of the data and the essay in the following 2 weeks.  

# Packages
```{r message=FALSE, warning=FALSE}
library(readr)
library(tidyverse)
library(tidymodels)
library(psych)
library(caret)
library(rpart)
library(rpart.plot)
library(corrplot)
library(RColorBrewer)
library(labelled)
library(ggplot2)
library(ggforce)
```


# Load Data
```{r}
df1k <- read.csv("https://raw.githubusercontent.com/gabbypaola/DATA622/main/HW%201/1000%20Sales%20Records.csv")
df100k <- read.csv("https://raw.githubusercontent.com/gabbypaola/DATA622/main/HW%201/100000%20Sales%20Records.csv")
```

# Exploratory Data Analysis

First, exploratory data analysis is conducted to get acquainted with the two Sales datasets selected from https://excelbianalytics.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/. 

## Dimensions, Variable Types, Labels, Levels, and Frequencies

The dimensions of the 1k data set are checked. The dataset contains 1000 rows, and 14 columns, or predictors. There are 7 character, 2 integer, and 5 double type variables. The `glimpse()` output shows us the first few observation of each variable. There are two geographical variables one for Region, and another for each specific country. Item.Type refers to the type of items sold, Sales.Channel refers to the sales method, whether the sale was conducted online or offline, meaning an in-store purchase.  

The `look_for()` function from the labelled package shows us each variable, their label (if available), column type, as well as the values for any factor type variables. The function produces no labels, and no values as none of the variables are coded as factor types. 

^[https://cran.r-project.org/web/packages/labelled/vignettes/intro_labelled.html]
```{r}
print("Dimensions")
glimpse(df1k)
print("Labels")
look_for(df1k)
```

Next, using the `unique()` and `length()` functions are used to investigate the geographic variables. There are 7 Regions and 185 countries spanning the 1k dataset.
```{r}
unique(df1k$Region)
length(unique(df1k$Country))
```

The `table()` function is used to create contingency, or frequency tables of some of the character variables, Item.Type and Sales.Channel. The sorted frequency table shows the top three items of the total 12 types in the 1k dataset are beverages, vegetables, followed by office supplies.
```{r}
table(df1k$Item.Type)[order(table(df1k$Item.Type),decreasing = TRUE)]
table(df1k$Sales.Channel)
```

Following the EDA for the 1k dataset, is the EDA for the 100k dataset. The 100k dataset contains 100,000 rows and 14 columns. Similar to the 1k dataset, in the 100k dataset there are 7 character, 2 integer, and 5 double type variables. The `glimpse()` output shows us the first few observation of each variable. There are two geographical variables one for Region, and another for each specific country. Item.Type refers to the type of items sold, Sales.Channel refers to the sales method, whether the sale was conducted online or offline, meaning an in-store purchase.

The `look_for()` function from the labelled package shows us each variable, their label (if available), column type, as well as the values for any factor type variables. The function produces no labels, and no values as none of the variables are coded as factor types. 
```{r}
print("Dimensions")
glimpse(df100k)
print("Labels")
look_for(df100k)
```
As above, using the `unique()` and `length()` functions are used to investigate the geographic variables. There are also 7 Regions and 185 countries spanning the 100k dataset.
```{r}
unique(df100k$Region)
length(unique(df100k$Country))
```

Once again the `table()` function is used to create contingency, or frequency tables of some of the character variables, Item.Type and Sales.Channel in the 100k dataset. The sorted frequency table shows the top three items of the total 12 types in the 100k dataset are office supplies, cereal, and baby food.
```{r}
table(df100k$Item.Type)[order(table(df100k$Item.Type),decreasing = TRUE)]
table(df100k$Sales.Channel)
```

## Variable Dependencies and Definitions

Both datasets contain the same information, only differing in the amount of observations. As such, below are the variable dependencies and definitions that are applicable to both the 1k and 100k dataset.

`Total.Cost` = `Units.Sold` * `Unit.Cost`  
`Total.Revenue` = `Units.Sold` * `Unit.Price`  
`Total.Profit` = `Total.Revenue` - `Total.Cost` (where `Total.Cost` and `Total.Revenue` depend on `Units.Sold`, `Unit.Cost`, and `Unit.Price`)

`Order.Priority`: C(Critical), H(High), M(Medium), and L(Low)

## Variable Type Conversions

Variable types are converted to ensure the models created use the correct type of variables. Variable conversion also aids in the preprocessing by ensuring originally numeric variables such as `Order.ID` are not inadvertently normalized or used in the modelling stage. There are a number of advantages to converting categorical variables to factor variables. Perhaps the most important advantage is that they can be used in statistical modeling where they will be implemented correctly, i.e., they will then be assigned the correct number of degrees of freedom. ^[https://stats.oarc.ucla.edu/r/modules/factor-variables/#:~:text=Perhaps%20the%20most%20important%20advantage,many%20different%20types%20of%20graphics.]

```{r}
df1k[['Order.Date']] <- as.Date(df1k[['Order.Date']], "%m/%d/%Y")
df1k[['Ship.Date']] <- as.Date(df1k[['Ship.Date']], "%m/%d/%Y")

df100k[['Order.Date']] <- as.Date(df100k[['Order.Date']], "%m/%d/%Y")
df100k[['Ship.Date']] <- as.Date(df100k[['Ship.Date']], "%m/%d/%Y")

df1k[['Sales.Channel']] <- as.factor(df1k[['Sales.Channel']])
df100k[['Sales.Channel']] <- as.factor(df100k[['Sales.Channel']])

df1k[['Order.Priority']] <- as.factor(df1k[['Order.Priority']])
df100k[['Order.Priority']] <- as.factor(df100k[['Order.Priority']])

df1k[['Item.Type']] <- as.factor(df1k[['Item.Type']])
df100k[['Item.Type']] <- as.factor(df100k[['Item.Type']])

df1k[['Region']] <- as.factor(df1k[['Region']])
df100k[['Region']] <- as.factor(df100k[['Region']])

df1k[['Country']] <- as.factor(df1k[['Country']])
df100k[['Country']] <- as.factor(df100k[['Country']])

df1k[['Order.ID']] <- as.character(df1k[['Order.ID']])
df100k[['Order.ID']] <- as.character(df100k[['Order.ID']])
```

```{r}
levels(df1k$Sales.Channel)
levels(df100k$Sales.Channel)
```


## Missing Data

Next the data is checked for any missing values.

The below shows no missing values for the 1k dataset. 

```{r}
colSums(is.na(df1k))
```

As in the 1k dataset, there are no missing values for the 100k dataset. 

```{r}
colSums(is.na(df100k))
```






## Distributions
```{r}
#select numeric columns 1k
df1k_n <- df1k %>% 
   keep(is.numeric) 
  #  %>% select(-Order.ID)

#select numeric columns 100k
df100k_n <- df100k %>% 
  keep(is.numeric) 
# %>% 
#   select(-Order.ID)
```

```{r message=FALSE, warning=FALSE}
#stats
describe(df1k_n, fast=TRUE) %>% 
  select(c(-vars,-n))

#distribution
df1k_n %>% 
  gather(variable, value, 1:6) %>%
  ggplot(aes(value)) +
    facet_wrap(~variable, scales = "free") +
    geom_density(fill = "steelblue", alpha=0.9, color="steelblue") +
    geom_histogram(aes(y=after_stat(density)), alpha=0.2, fill = "lightblue", 
                   color="lightblue", position="identity", bins = 40) +
    theme_minimal()
```

```{r message=FALSE, warning=FALSE}
#stats
describe(df100k_n, fast=TRUE) %>% 
  select(c(-vars,-n))

#distributions
df100k_n %>% 
  gather(variable, value, 1:6) %>%
  ggplot(aes(value)) +
    facet_wrap(~variable, scales = "free") +
    geom_density(fill = "steelblue", alpha=0.9, color="steelblue") +
    geom_histogram(aes(y=after_stat(density)), alpha=0.2, fill = "lightblue", 
                   color="lightblue", position="identity",bins = 40) +
    theme_minimal()
```
  

## Correlations

Subsequently, the correlations between numeric variables are examined. It is important to investigate the correlations between the independent variables to avoid multicollinearity. Multicollinearity occurs when two or more independent variables are highly correlated to one another. When two (or more) independent variables are highly correlated one cannot individually determine the impact of individual variables on the dependent variable.  

Multicollinearity can be a problem in a regression model when using algorithms such as OLS (ordinary least squares). This is because the estimated regression coefficients become unstable and difficult to interpret in the presence of multicollinearity.  

When multicollinearity is present, the estimated regression coefficients may become large and unpredictable, leading to unreliable inferences about the effects of the predictor variables on the response variable. Therefore, it is important to check for multicollinearity and consider using other regression techniques that can handle this problem, such as ridge regression or principal component regression or make a decision about dropping highly correlated independent variables.

```{r}
cor(df1k_n) %>%
  corrplot(tl.col = 'black', diag=FALSE, type="lower", 
           order="hclust", addCoef.col = "black",
           #mar argument fixes title positioning
           title="1k dataset Correlations", mar=c(0,0,1,0), 
           col=brewer.pal(n=10, name="RdYlBu"))

```

```{r}
cor(df100k_n) %>%
  corrplot(tl.col = 'black', diag=FALSE, type="lower", 
           order="hclust", addCoef.col = "black",
           title="100k dataset Correlations",mar=c(0,0,1,0),
           col=brewer.pal(n=10, name="RdYlBu"))

```

The correlation plots for both the 1k and 100k dataset show nearly identical correlation coefficients. Variables with weak correlation (i.e., 0<=|r|<0.3) ^[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4888898/] between Unit.Price, Unit.Cost, and Units.Sold. Moderate correlation (i.e., 0.3â‰¤|r|<0.7) can be seen between Total.Profit, Total.Revenue, Total.Cost and Units.Sold as well as between Unit.Price, Unit.Cost and Total.Profit. High correlation (i.e., |r|>=7) can be seen in the remaining pairs, especially between Unit.Price and Unit.Cost, Total.Profit and Total.Revenue, Total.Cost and Total.Revenue.

## VIF Scores

Another detection method for multicoliniarity is through the use of the VIF (Variance Inflation Factor) score.  The variance inflation factor (or VIF), measures how much the variance of a regression coefficient is inflated due to multicollinearity in the model. The smallest possible value of VIF is one (absence of multicollinearity). As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. ^[http://www.sthda.com/english/articles/39-regression-model-diagnostics/160-multicollinearity-essentials-and-vif-in-r/]

```{r}
set.seed(145)

training.samples <- df1k_n$Total.Revenue %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- df1k_n[training.samples, ]
test.data <- df1k_n[-training.samples, ]
```

```{r}
# Build the model
#train.data1 <-train.data #%>% select(c(-Total.Profit,-Total.Cost, -Unit.Price)) 
model1<- lm(Total.Revenue~., data=train.data )

summary(model1)
# Make predictions
predictions <- model1 %>% predict(test.data)
# Model performance
data.frame(
  RMSE = RMSE(predictions, test.data$Total.Revenue),
  R2 = R2(predictions, test.data$Total.Revenue)
)
```

As expected from the correlation plot, multicoliniarity is detected through the resulting VIF scores for the 1k dataset. Four of the five numeric variables have VIF scores higher than 10, two of which are exorbitantly high with VIF scores of 172 as seen below.

```{r}
car::vif(model1)
```

Similarly, the VIF scores are calculated for the 100k dataset.

```{r}
set.seed(145)

training.samples2 <- df100k_n$Total.Revenue %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data2  <- df100k_n[training.samples2, ]
test.data2 <- df100k_n[-training.samples2, ]
```

```{r}
# Build the model
#%>% select(c(-Total.Profit,-Total.Cost, -Unit.Price)) 
model2<- lm(Total.Revenue~., data=train.data2 )

summary(model2)
# Make predictions
predictions2 <- model2 %>% predict(test.data2)
# Model performance
data.frame(
  RMSE = RMSE(predictions2, test.data2$Total.Revenue),
  R2 = R2(predictions2, test.data2$Total.Revenue)
)
```

As in the VIF scores for the 1k dataset, the 100k dataset exhibits high VIF values, confirming the multicoliniairty detected between the predictor variables in the correlation plot.

```{r}
car::vif(model1)
```

In both the 1k and 100k datasets, the only variable not exceeding the recommended VIF score of 5 is the Units.Sold variable. As such, it will be the only numeric variable used in the modeling.


# Preprocessing
## Normalization

Next, the data is normalized in preparation for modelling. ^[https://www.statology.org/how-to-normalize-data-in-r/] When variables are measured at different scales, they often do not contribute equally to the analysis. For example, if the values of one variable range from 0 to 100,000 and the values of another variable range from 0 to 100, the variable with the larger range will be given a larger weight in the analysis. By normalizing the variables, we can be sure that each variable contributes equally to the analysis.

```{r}
#define Min-Max normalization function
min_max_norm <- function(x) {
    (x - min(x)) / (max(x) - min(x))
}

#apply Min-Max normalization to numeric columns
df1k_norm <- as.data.frame(lapply(df1k_n, min_max_norm))

df100k_norm <- as.data.frame(lapply(df100k_n, min_max_norm))
```

```{r message=FALSE, warning=FALSE}
#stats
describe(df1k_norm, fast=TRUE) %>% 
  select(c(-vars,-n))

#distribution
df1k_norm %>% 
  gather(variable, value, 1:6) %>%
  ggplot(aes(value)) +
    facet_wrap(~variable, scales = "free") +
    geom_density(fill = "steelblue", alpha=0.9, color="steelblue") +
    geom_histogram(aes(y=after_stat(density)), alpha=0.2, fill = "lightblue", 
                   color="lightblue", position="identity", bins = 40) +
    theme_minimal()
```

```{r message=FALSE, warning=FALSE}
#stats
describe(df100k_norm, fast=TRUE) %>% 
  select(c(-vars,-n))

#distribution
df100k_norm %>% 
  gather(variable, value, 1:6) %>%
  ggplot(aes(value)) +
    facet_wrap(~variable, scales = "free") +
    geom_density(fill = "steelblue", alpha=0.9, color="steelblue") +
    geom_histogram(aes(y=after_stat(density)), alpha=0.2, fill = "lightblue", 
                   color="lightblue", position="identity", bins = 40) +
    theme_minimal()
```

# Models
## Simple Linear Regression

The first model created is a simple linear regression using the normalized numeric data for the 1k dataset.

First the data is split into a train and test sets.
```{r}
set.seed(146)

training.samples3 <- df1k_norm$Total.Revenue %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data3  <- df1k_norm[training.samples3, ]
test.data3 <- df1k_norm[-training.samples3, ]
```

Next, the model is run using the predictor variable, `Units.Sold` identified in the EDA stage from the VIF Scores calculated.

The model performs terribly with an R-squared value of .268
```{r}
model3<- lm(Total.Revenue~Units.Sold, data=train.data3 )

summary(model3)
# Make predictions
predictions3 <- model3 %>% predict(test.data3)

# Model performance
data.frame(
  RMSE = RMSE(predictions3, test.data3$Total.Revenue),
  R2 = R2(predictions3, test.data3$Total.Revenue)
)
```

Another simple regression model is created, this time with the 100k dataset using the same predictor and response variables as in the model created using the 1k dataset.

```{r}
set.seed(146)

training.samples4 <- df100k_norm$Total.Revenue %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data4  <- df1k_norm[training.samples4, ]
test.data4 <- df1k_norm[-training.samples4, ]
```

```{r}
model4<- lm(Total.Revenue~Units.Sold, data=train.data4)

summary(model4)
# Make predictions
predictions4 <- model4 %>% predict(test.data4)

# Model performance
data.frame(
  RMSE = RMSE(predictions4, test.data4$Total.Revenue),
  R2 = R2(predictions4, test.data4$Total.Revenue)
)
```
As in the simple regression model using the 1k dataset, the 100k dataset doesn't perform well. It has an accuracy of 26% which is nearly 1% worse than the model performance for the 1k dataset.

## Multiple Linear Regression

For the next model, additional predictor variables are incorporated in the `lm()` function to generate a multiple linear regression model for both the 1k  and 100k datasets.

First step is to narrow down the variables of the 1k dataset to be used and then preprocess the numeric values to create the model.
```{r}
df1k2 <- df1k %>% 
  # purrr::discard(is.numeric) %>% 
  select(-c(Country, Unit.Cost, Unit.Price, Total.Cost, Total.Profit, Order.ID))

preproc1 <-preProcess(df1k2, method=c("center", "scale"))
norm1 <- predict(preproc1,df1k2)
head(norm1)
```

```{r}
set.seed(146)

training.samples5 <- norm1$Total.Revenue %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data5  <- norm1[training.samples5, ]
test.data5 <- norm1[-training.samples5, ]
```


```{r}
model5<- lm(Total.Revenue~., data=train.data5 )

summary(model5)
# Make predictions
predictions5 <- model5 %>% predict(test.data5)

# Model performance
data.frame(
  RMSE = RMSE(predictions5, test.data5$Total.Revenue),
  R2 = R2(predictions5, test.data5$Total.Revenue)
)
```
A significant improvement in the accuracy of the model is shown by the R squared value which is now 0.81.


Next is the multiple linear regression model for the 100k dataset. 
```{r}
df100k2 <- df100k %>% 
  # purrr::discard(is.numeric) %>% 
  select(-c(Country, Unit.Cost, Unit.Price, Total.Cost, Total.Profit, Order.ID))

preproc2 <-preProcess(df100k2, method=c("center", "scale"))
norm2 <- predict(preproc2,df100k2)
head(norm2)
```

```{r}
set.seed(147)

training.samples6 <- norm2$Total.Revenue %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data6  <- norm2[training.samples6, ]
test.data6 <- norm2[-training.samples6, ]
```

```{r}
model6<- lm(Total.Revenue~., data=train.data6 )

summary(model6)
# Make predictions
predictions6 <- model6 %>% predict(test.data6)

# Model performance
data.frame(
  RMSE = RMSE(predictions6, test.data6$Total.Revenue),
  R2 = R2(predictions6, test.data6$Total.Revenue)
)
```

# Essay
**Answer questions such as:**  
1. Are the columns of your data correlated?
Yes, the numeric variables in both the 1k and 100k dataset are highly correlated which were verified using correlation plots as well as VIF scores. A majority of the numerical variables are highly correlated with exception to `Units.Sold` with correlations at 0 with `Unit.Cost` and `Unit.Price`. `Units.Sold` highest correlation is with `Total.Profit` which makes sense as `Total.Profit` has dependency on `Units.Sold`, `Total.Profit` = `Total.Revenue` - `Total.Cost` (where `Total.Cost` and `Total.Revenue` depend on `Units.Sold`, `Unit.Cost`, and `Unit.Price`). The highest correlations exist between `Unit.Cost` and `Unit.Price`, `Total.Revenue` and `Total.Profit`, as well as `Total.Revenue` and `Total.Cost`.  
2. Are there labels in your data? Did that impact your choice of algorithm?  
No, both datasets do not contain labels in their metadata as seen by the output of the `look_for()` function from the `labelled` package.  
3. What are the pros and cons of each algorithm you selected?   
The pro of the simple linear regression algorithm was it's easy interpretation, however it's major con is its poor accuracy.

In contrast, the multiple linear regression has much better performance, however its interpretability is slightly less than that of the simple linear regression given the added predictor variables which include factors of multiple levels.
  
4. How your choice of algorithm relates to the datasets (was your choice of algorithm impacted by the datasets you chose)?   

  
5. Which result will you trust if you need to make a business decision?  
6. Do you think an analysis could be prone to errors when using too much data, or when using the least amount possible?  
7. How does the analysis between data sets compare?  
Develop your exploratory analysis of the data and the essay in the following 2 weeks.  


# References

<!------- Below is for removing excessive space in Rmarkdown | HTML formatting -------->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>