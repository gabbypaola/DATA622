---
title: "DATA622 | Machine Learning and Big Data"
author: "Gabriella Martinez"
date: "3/26/2023"
output:
      html_document:
        toc: yes
        toc_float: yes
        theme: yeti
        highlight: kate
        font-family: "Arial"
        code_folding: show
---

# Assignment 2
**Pre-work**  
1. Read this [blog](https://decizone.com/blog/the-good-the-bad-the-ugly-of-using-decision-trees), which shows some of the issues with decision trees  
2. Choose a dataset from a source in Assignment #1, or another dataset of your choice.  

**Assignment work**  
1. Based on the latest topics presented, choose a dataset of your choice and create a Decision Tree where you can solve a classification or regression problem and predict the outcome of a particular feature or detail of the data used.  
2. Switch variables to generate 2 decision trees and compare the results. Create a random forest for regression and analyze the results.  
3. Based on real cases where decision trees went wrong, and 'the bad & ugly' aspects of decision trees, how can you change this perception when using the decision tree you created to solve a real problem?

**Deliverable**  
1. Essay (minimum 500 words)  
Write a short essay explaining your analysis, and how you would address the concerns in the blog (listed in pre-work)  
2. Exploratory Analysis using R or Python (submit code + errors + analysis as notebook or copy/paste to document)

# Packages
```{r message=FALSE, warning=FALSE}
library(readr)
library(tidyverse)
library(tidymodels)
library(psych)
library(caret)
library(rpart)
library(rpart.plot)
library(corrplot)
library(RColorBrewer)
library(labelled)
library(ggplot2)
library(ggforce)
library(pROC)
library(mlbench)
library(ModelMetrics)
library(randomForest)
library(e1071)
```

# Load Data
```{r}
df1k <- read.csv("https://raw.githubusercontent.com/gabbypaola/DATA622/main/HW%201/1000%20Sales%20Records.csv")
```

# Exploratory Data Analysis

First, exploratory data analysis is conducted to get acquainted with the two Sales datasets selected from https://excelbianalytics.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/. 

## Dimensions, Variable Types, Labels, Levels, and Frequencies
The 1k dataset contains 100,000 rows and 14 columns. Similar to the 1k dataset, in the 1k dataset there are 7 character, 2 integer, and 5 double type variables. The `glimpse()` output shows us the first few observation of each variable. There are two geographical variables one for Region, and another for each specific country. `Item.Type` refers to the type of items sold, `Sales.Channel` refers to the sales method, whether the sale was conducted online or offline, meaning an in-store purchase.

The `look_for()` function from the labelled package shows us each variable, their label (if available), column type, as well as the values for any factor type variables. The function produces no labels, and no values as none of the variables are coded as factor types. 

```{r}
print("Dimensions")
glimpse(df1k)
print("Labels")
look_for(df1k)
```

As above, using the `unique()` and `length()` functions are used to investigate the geographic 
variables. There are also 7 Regions and 185 countries spanning the 1k dataset.

```{r}
unique(df1k$Region)
length(unique(df1k$Country))
```

Once again the `table()` function is used to create contingency, or frequency tables of some of the character variables, Item.Type and Sales.Channel in the 1k dataset. The sorted frequency table shows the top three items of the total 12 types in the 1k dataset are office supplies, cereal, and baby food.

```{r}
table(df1k$Item.Type)[order(table(df1k$Item.Type),decreasing = TRUE)]
table(df1k$Sales.Channel)
```

## Variable Dependencies and Definitions

Below are the variable dependencies and definitions that are applicable to 1k dataset.

`Total.Cost` = `Units.Sold` * `Unit.Cost`  
`Total.Revenue` = `Units.Sold` * `Unit.Price`  
`Total.Profit` = `Total.Revenue` - `Total.Cost` (where `Total.Cost` and `Total.Revenue` depend on `Units.Sold`, `Unit.Cost`, and `Unit.Price`)

`Order.Priority`: C(Critical), H(High), M(Medium), and L(Low)

## Missing Data

Next the data is checked for any missing values.

The below shows no missing values for the 1k dataset. 

```{r}
colSums(is.na(df1k))
```

## Distributions

From the `describe()` output for the 1k dataset, it is noted that the numeric variables also have wide ranges. This will be taken care of through normalization to scale the variables in the preprocessing stage prior to running models. Futhermore, through the use of the graphs created for each variable, it is noted that `Total.Cost`, `Total.Profit`, and `Total.Revenue` are right skewed and the distributions for `Unit.Cost`, `Unit.Price`, and `Units.Sold` are multimodal. `Unit.Cost` shows six modes, `Unit.Price` shows five modes, and `Units.Sold` show four modes. 

```{r message=FALSE, warning=FALSE}
#select numeric columns 1k
df1k_n <- df1k %>% 
  keep(is.numeric) 

#stats
describe(df1k_n, fast=TRUE) %>% 
  select(c(-vars,-n))

#distributions
df1k_n %>% 
  gather(variable, value, 1:6) %>%
  ggplot(aes(value)) +
    facet_wrap(~variable, scales = "free") +
    geom_density(fill = "steelblue", alpha=0.9, color="steelblue") +
    geom_histogram(aes(y=after_stat(density)), alpha=0.2, fill = "lightblue", 
                   color="lightblue", position="identity",bins = 40) +
    theme_minimal()
```

## Correlations

Subsequently, the correlations between numeric variables are examined. It is important to investigate the correlations between the independent variables to avoid multicollinearity. Multicollinearity occurs when two or more independent variables are highly correlated to one another. When two (or more) independent variables are highly correlated one cannot individually determine the impact of individual variables on the dependent variable.  

Multicollinearity can be a problem in a regression model when using algorithms such as OLS (ordinary least squares). This is because the estimated regression coefficients become unstable and difficult to interpret in the presence of multicollinearity.  

When multicollinearity is present, the estimated regression coefficients may become large and unpredictable, leading to unreliable inferences about the effects of the predictor variables on the response variable. Therefore, it is important to check for multicollinearity and consider using other regression techniques that can handle this problem, such as ridge regression or principal component regression or make a decision about dropping highly correlated independent variables entirely.

Variables with weak correlation (i.e., 0<=|r|<0.3) ^[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4888898/] between `Unit.Price`,` Unit.Cost`, and `Units.Sold`. Moderate correlation (i.e., 0.3≤|r|<0.7) can be seen between `Total.Profit`, `Total.Revenue`, `Total.Cost` and `Units.Sold` as well as between `Unit.Price`, `Unit.Cost` and `Total.Profit`. High correlation (i.e., |r|>=0.7) can be seen in the remaining pairs, especially between `Unit.Price` and `Unit.Cost`, `Total.Profit` and `Total.Revenue`, `Total.Cost` and `Total.Revenue`.

```{r}
cor(df1k_n) %>%
  corrplot(tl.col = 'black', diag=FALSE, type="lower", 
           order="hclust", addCoef.col = "black",
           title="1k dataset Correlations",mar=c(0,0,1,0),
           col=brewer.pal(n=10, name="RdYlBu"))

```

## VIF Scores

Another detection method for multicoliniarity is through the use of the VIF (Variance Inflation Factor) score. Although correlation matrix and scatter plots can also be used to find multicollinearity, their findings only show the bivariate relationship between the independent variables. VIF is preferred as it can show the correlation of a variable with a group of other variables. The variance inflation factor (or VIF), measures how much the variance of a regression coefficient is inflated due to multicollinearity in the model. The smallest possible value of VIF is one (absence of multicollinearity). As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. ^[http://www.sthda.com/english/articles/39-regression-model-diagnostics/160-multicollinearity-essentials-and-vif-in-r/]

Below the VIF scores are calculated for the 1k dataset.

```{r}
set.seed(145)

training.samples0 <- df1k_n$Total.Revenue %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- df1k_n[training.samples0, ]
test.data <- df1k_n[-training.samples0, ]

# Build the model
#%>% select(c(-Total.Profit,-Total.Cost, -Unit.Price)) 
model<- lm(Total.Revenue~., data=train.data )

summary(model)
# Make predictions
predictions0 <- model %>% predict(test.data)
# Model performance
data.frame(
  MAE = mae(predictions0, test.data$Total.Revenue),
  RMSE = RMSE(predictions0, test.data$Total.Revenue),
  R2 = R2(predictions0, test.data$Total.Revenue)
)
```

The 1k dataset exhibits high VIF values, confirming the multicoliniairty detected between the predictor variables in the correlation plot.

```{r}
car::vif(model)
```

The only variable not exceeding the recommended VIF score of 5 is the `Units.Sold` variable.

# Preprocessing

## Variable Type Conversions

Variable types are converted to ensure the models created use the correct type of variables. Variable conversion also aids in the preprocessing by ensuring originally numeric variables such as `Order.ID` are not inadvertently normalized or used in the modelling stage. There are a number of advantages to converting categorical variables to factor variables. Perhaps the most important advantage is that they can be used in statistical modeling where they will be implemented correctly, i.e., they will then be assigned the correct number of degrees of freedom. ^[https://stats.oarc.ucla.edu/r/modules/factor-variables/#:~:text=Perhaps%20the%20most%20important%20advantage,many%20different%20types%20of%20graphics.]

```{r}
df1k[['Order.Date']] <- as.Date(df1k[['Order.Date']], "%m/%d/%Y")
df1k[['Ship.Date']] <- as.Date(df1k[['Ship.Date']], "%m/%d/%Y")

df1k[['Sales.Channel']] <- as.factor(df1k[['Sales.Channel']])

df1k[['Order.Priority']] <- as.factor(df1k[['Order.Priority']])

df1k[['Item.Type']] <- as.factor(df1k[['Item.Type']])

df1k[['Region']] <- as.factor(df1k[['Region']])

df1k[['Country']] <- as.factor(df1k[['Country']])

df1k[['Order.ID']] <- as.character(df1k[['Order.ID']])
```

```{r}
levels(df1k$Sales.Channel)
```

## Scaling

Next, the data is scaled in preparation for modelling. ^[https://www.statology.org/how-to-normalize-data-in-r/] When variables are measured at different scales, they often do not contribute equally to the analysis. For example, if the values of one variable range from 0 to 100,000 and the values of another variable range from 0 to 100, the variable with the larger range will be given a larger weight in the analysis. By scaling the variables, we can be sure that each variable contributes equally to the analysis and reduce the variance effect.

```{r}
#apply preprocessing and generate transformed values
df1k_norm<-predict(preProcess(df1k, method=c("center", "scale")),df1k)

```

```{r message=FALSE, warning=FALSE}
#stats
df1k_norm %>% 
  keep(is.numeric) %>%  
  describe(fast=TRUE) %>% 
  select(-c(vars,n))

#distribution
df1k_norm %>% 
  keep(is.numeric) %>%  
  gather(variable, value, 1:6) %>%
  ggplot(aes(value)) +
    facet_wrap(~variable, scales = "free") +
    geom_density(fill = "steelblue", alpha=0.9, color="steelblue") +
    geom_histogram(aes(y=after_stat(density)), alpha=0.2, fill = "lightblue", 
                   color="lightblue", position="identity", bins = 40) +
    theme_minimal()
```

## Feature Selection

Feature Selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data. It is primarily focused on removing non-informative or redundant predictors from the model. 

```{r}
df1k_norm <- df1k_norm %>% 
  select(-c(Country,Order.ID,)) #Unit.Price, Unit.Cost, Total.Cost, Total.Profit
```


# Models

## Regression Decision Tree 1

```{r}
set.seed(1234)

df1k_norm1 <- df1k_norm 

#split
training.samples <- df1k_norm1$Total.Revenue %>% 
  createDataPartition(p = 0.8, list = FALSE)

train  <- df1k_norm1[training.samples, ]
test <- df1k_norm1[-training.samples, ]

#train using rpart, cp- complexity, smaller # = more complexity, 
#method- anova is for regression
tree <- rpart(Total.Revenue ~., data = train, cp = 0.004,  method = 'anova')

#visualize
rpart.plot(tree)
print(tree)
```




### Predictions
```{r message=FALSE, warning=FALSE, results='hide'}
predictions <- predict(tree, newdata = test) %>% 
  bind_cols(test)

predictions$...1 <- as.numeric(predictions$...1)

```

### Model Performance
```{r}
a <- data.frame(Model = "Decision Tree 1",
#mean absolute error
MAE = mae(predictions$Total.Revenue, predictions$...1),
#rmse Root Mean Squared Error
RMSE = rmse(predictions$Total.Revenue, predictions$...1),
#r squared
R2 = R2(predictions$Total.Revenue, predictions$...1)
)
```

## Regression Decision Tree 2
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
set.seed(123456)
#colnames(df1k_norm1)

df1k_norm3 <- df1k_norm %>%
  select(-c("Unit.Price","Unit.Cost","Total.Cost", "Total.Profit"))

#split
training.samples3 <- df1k_norm3$Total.Revenue %>% 
  createDataPartition(p = 0.8, list = FALSE)

train3  <- df1k_norm3[training.samples3, ]
test3 <- df1k_norm3[-training.samples3, ]

#train using rpart, cp- complexity, smaller # = more complexity, 
#method- anova is for regression
tree3 <- rpart(Total.Revenue ~., data = train3, cp = 0.004, method = 'anova')

#visualize
rpart.plot(tree3)
print(tree3)

```

### Predictions
```{r message=FALSE, warning=FALSE, results='hide'}
predictions3 <- predict(tree3, newdata = test3) %>% 
  bind_cols(test3)

predictions3$...1 <- as.numeric(predictions3$...1)

```

### Model Performance
```{r}
b <- data.frame(Model = "Decision Tree 2",
#mean absolute error
MAE = mae(predictions3$Total.Revenue, predictions3$...1),
#rmse Root Mean Squared Error
RMSE = rmse(predictions3$Total.Revenue, predictions3$...1),
#r squared
R2 = R2(predictions3$Total.Revenue, predictions3$...1)
)
```



## Random Forest Regression Tree

```{r}
set.seed(201)
rf <- randomForest(formula = Total.Revenue ~ ., 
                   data = train, importance=TRUE)

rf

ImpData <- as.data.frame(importance(rf))
ImpData$Var.Names <- row.names(ImpData)

ggplot(ImpData, aes(x=Var.Names, y=`%IncMSE`)) +
  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color="skyblue") +
  geom_point(aes(size = IncNodePurity),  color="steelblue", alpha=1) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )

```

### Predictions

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
predictions4 <- predict(rf, newdata = test) %>% 
  bind_cols(test)

predictions4$...1 <- as.numeric(predictions4$...1)
```

### Model Performance

```{r}
c <- data.frame(Model = "Random Forest",
#mean absolute error
MAE = mae(predictions4$Total.Revenue, predictions4$...1),
#rmse Root Mean Squared Error
RMSE = rmse(predictions4$Total.Revenue, predictions4$...1),
#r squared
R2 = R2(predictions4$Total.Revenue, predictions4$...1)
)

c
```


## Tuned Random Forest Regression Tree

```{r}
set.seed(908)

train1 <- train %>% 
  select(-Total.Revenue)

bestmtry <- tuneRF(train1,train$Total.Revenue, stepFactor = 2, improve = 0.01,
                   trace=T, plot= T, doBest=TRUE, importance=TRUE)

bestmtry

#importance(bestmtry)

# Get variable importance from the model fit
ImpData <- as.data.frame(importance(bestmtry))
ImpData$Var.Names <- row.names(ImpData)

ggplot(ImpData, aes(x=Var.Names, y=`%IncMSE`)) +
  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color="skyblue") +
  geom_point(aes(size = IncNodePurity), color="steelblue", alpha=1) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )
```

### Predictions

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
predictions5 <- predict(bestmtry, newdata = test) %>% 
  bind_cols(test)

predictions5$...1 <- as.numeric(predictions5$...1)
```

### Model Performance

```{r}
d <- data.frame(Model = "Tuned Random Forest",
#mean absolute error
MAE = mae(predictions5$Total.Revenue, predictions5$...1),
#rmse Root Mean Squared Error
RMSE = rmse(predictions5$Total.Revenue, predictions5$...1),
#r squared
R2 = R2(predictions5$Total.Revenue, predictions5$...1)
)

d
```

# Essay

The analysis for assignment 2 builds upon the analysis conducted for assignment 1. The 1k dataset initially used for assignment 1 was recycled along with the exploratory data analysis and the preprocessing steps. The purpose of reusing the data set was to see how different models interact with the same dataset. The exploratory data analysis step was used to check the dimensions, variable types, missing data, distributions, and correlations. 

The numeric variables in the 1k dataset are highly correlated and were verified using correlation plots as well as VIF scores. The two variable relationships that have a 0 correlation are between `Units.Sold` and `Unit.Price`, as well as between `Units.Sold` and `Unit.Cost`. Moderate correlations (i.e., 0.3≤|r|<0.7) exist between the following pairs: `Units.Sold` and `Total.Profit`, `Units.Sold` and `Total.Revenue`, `Units.Sold` and `Total.Cost`, `Unit.Price` and `Total.Profit`, and `Unit.Cost` and `Total.Profit`. The highest correlations with values at .99 exist between `Unit.Price` and `Unit.Cost` as well as `Total.Cost` and `Total.Revenue`. The remaining six numeric variable relationships have high correlations (i.e., |r|>=0.7). Multicollinearity was expected between the numeric variables given the variable dependencies noted earlier in the exploratory data analysis portion, however both the correlation plots and the VIF scores confirmed this expectation.  

The preprocessing stage uses the insights gained on the data in the exploratory data analysis stage to manipulate and drop data before it is used in order to ensure or enhance performance. In checking the variable types it was noted that variable type conversions would be required in the preprocessing stage of the assignment. Variable types are converted to ensure the models created use the correct type of variables. Variable conversion also aids in the preprocessing by ensuring originally numeric variables such as `Order.ID` are not inadvertently normalized or used in the modeling stage. 

Next, from the summary statistics and distribution visualizations it is shown that scaling would need to be conducted in the preprocessing stage. When variables are measured at different scales, they often do not contribute equally to the analysis. For example, if the values of one variable range from 0 to 100,000 and the values of another variable range from 0 to 100, the variable with the larger range will be given a larger weight in the analysis. By scaling the variables, we can be sure that each variable contributes equally to the analysis and reduce the variance effect. The data is scaled using the `preProcess()` function from the caret library.

Following the exploratory data analysis and preprocessing stages comes the modeling stage. In  the first regression decision tree model all the predictor variables are used to model the `Total.Revenue` variable. The cp parameter is included to increase the complexity of the decision tree. The default results in a basic tree with few splits. From the high correlations between the numeric variables, it is expected the first variables the decision tree model uses will be among the numeric predictors. As expected, the top node splitting the data is `Total.Cost`, if `Total.Cost` is less than 0.61. After `Total.Cost`, the tree splits on `Total.Cost` again with different thresholds, `Total.Cost`<-0.31 and `Total.Cost` < 2.2.The first decision tree ends up with 7 decision nodes and 9 terminal nodes, using `Total.Cost` on 6 decision nodes and `Total.Profit` on one decision node. 

In contrast to the first decision tree model, feature selection is used on the input data. The variables showing high multicollinearity are removed from the dataset. Although decision trees do not require the removal of multicollinear variables, I wanted to compare the resulting decision tree model using the same predictors to the multiple regression model created in HW1. It is no surprise that the second decision tree model slightly outperformed the multiple linear regression model (RMSE= 0.41, R2= 0.81). While the second decision tree model slightly outperformed the multiple linear regression model, it did not outperform the first decision tree model created using all the multicollinear predictors. Although the second decision tree model didn’t have better metrics in comparison to the first decision tree model, it was still interesting to see how the categorical factor variables behave. The factor variable primarily used in the decision tree was `Item.Type`, where it is the root node and 4 decision nodes. `Item.Type` gets split up into different combinations in each decision node. The remaining 8 decision nodes were split on the only numerical predictor, `Units.Sold`.

Finally, for the random forest component of the assignment, two random forest models are created. One in which the `randomForest()` function is used without the use of hyperparameters, and another that uses the `tuneRF()` function with hyperparameters. It isn’t much of a surprise that the tuned random forest model slightly outperforms both decision tree models and the simple random forest model as noted by the metrics below.


```{r}
rbind(a,b,c,d)
```



# References

<!------- Below is for removing excessive space in Rmarkdown | HTML formatting -------->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>